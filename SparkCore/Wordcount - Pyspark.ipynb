{"cells":[{"cell_type":"markdown","source":["#**WordCount Example**\n\n###Goal:  Determine the most popular words in a given text file using Python and SQL"],"metadata":{}},{"cell_type":"markdown","source":["### ![](http://training.databricks.com/databricks_guide/downarrow.png) **Step 1**: Load text file from our [Hosted Datasets](https://docs.databricks.com/user-guide/faq/databricks-datasets.html).  **Shift-Enter Runs the code below.**"],"metadata":{}},{"cell_type":"code","source":["filePath = \"dbfs:/databricks-datasets/SPARK_README.md\" # path in Databricks File System\nlines = sc.textFile(filePath) # read the file into the cluster\nlines.take(10) # display first 10 lines in the file"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: [&#39;# Apache Spark&#39;,\n &#39;&#39;,\n &#39;Spark is a fast and general cluster computing system for Big Data. It provides&#39;,\n &#39;high-level APIs in Scala, Java, Python, and R, and an optimized engine that&#39;,\n &#39;supports general computation graphs for data analysis. It also supports a&#39;,\n &#39;rich set of higher-level tools including Spark SQL for SQL and DataFrames,&#39;,\n &#39;MLlib for machine learning, GraphX for graph processing,&#39;,\n &#39;and Spark Streaming for stream processing.&#39;,\n &#39;&#39;,\n &#39;&lt;http://spark.apache.org/&gt;&#39;]</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["###![](http://training.databricks.com/databricks_guide/downarrow.png) **Step 2**:  Inspect the number of partitions (workers) used to store the dataset"],"metadata":{}},{"cell_type":"code","source":["numPartitions = lines.getNumPartitions() # get the number of partitions\nprint (\"Number of partitions (workers) storing the dataset\", numPartitions)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of partitions (workers) storing the dataset 2\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["###![](http://training.databricks.com/databricks_guide/downarrow.png) **Step 3**:  Split each line into a list of words separated by a space from the dataset"],"metadata":{}},{"cell_type":"code","source":["words = lines.flatMap(lambda x: x.split(' ')) # split each line into a list of words\nwords.take(10) # display the first 10 words"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [&#39;#&#39;, &#39;Apache&#39;, &#39;Spark&#39;, &#39;&#39;, &#39;Spark&#39;, &#39;is&#39;, &#39;a&#39;, &#39;fast&#39;, &#39;and&#39;, &#39;general&#39;]</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["###![](http://training.databricks.com/databricks_guide/downarrow.png) **Step 4**:  Filter the list of words to exclude common stop words"],"metadata":{}},{"cell_type":"code","source":["stopWords = ['','a','*','and','is','of','the','a'] # define the list of stop words\nfilteredWords = words.filter(lambda x: x.lower() not in stopWords) # filter the words\nfilteredWords.take(10) # display the first 10 filtered words"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: [&#39;#&#39;,\n &#39;Apache&#39;,\n &#39;Spark&#39;,\n &#39;Spark&#39;,\n &#39;fast&#39;,\n &#39;general&#39;,\n &#39;cluster&#39;,\n &#39;computing&#39;,\n &#39;system&#39;,\n &#39;for&#39;]</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["###![](http://training.databricks.com/databricks_guide/downarrow.png) **Step 5**:  Cache the filtered dataset in memory to speed up future actions."],"metadata":{}},{"cell_type":"code","source":["filteredWords.cache() # cache filtered dataset into memory across the cluster worker nodes"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">14</span><span class=\"ansired\">]: </span>PythonRDD[39] at RDD at PythonRDD.scala:48\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["###![](http://training.databricks.com/databricks_guide/downarrow.png) **Step 6**:  Transform filtered words into list of (word,1) tuples for WordCount"],"metadata":{}},{"cell_type":"code","source":["word1Tuples = filteredWords.map(lambda x: (x, 1)) # map the words into (word,1) tuples\nword1Tuples.take(10) # display the (word,1) tuples"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: [(&#39;#&#39;, 1),\n (&#39;Apache&#39;, 1),\n (&#39;Spark&#39;, 1),\n (&#39;Spark&#39;, 1),\n (&#39;fast&#39;, 1),\n (&#39;general&#39;, 1),\n (&#39;cluster&#39;, 1),\n (&#39;computing&#39;, 1),\n (&#39;system&#39;, 1),\n (&#39;for&#39;, 1)]</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["###![](http://training.databricks.com/databricks_guide/downarrow.png) **Step 7**:  Aggregate the (word,1) tuples into (word,count) tuples"],"metadata":{}},{"cell_type":"code","source":["wordCountTuples = word1Tuples.reduceByKey(lambda x, y: x + y) # aggregate counts for each word\nwordCountTuples.take(10) # display the first 10 (word,count) tuples"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">16</span><span class=\"ansired\">]: </span>\n[(u&apos;when&apos;, 1),\n (u&apos;&quot;local&quot;&apos;, 1),\n (u&apos;through&apos;, 1),\n (u&apos;computation&apos;, 1),\n (u&apos;using:&apos;, 1),\n (u&apos;guidance&apos;, 2),\n (u&apos;Scala,&apos;, 1),\n (u&apos;environment&apos;, 1),\n (u&apos;only&apos;, 1),\n (u&apos;rich&apos;, 1)]\n</div>"]}}],"execution_count":15}],"metadata":{"name":"Wordcount - Pyspark","notebookId":3891688439950040},"nbformat":4,"nbformat_minor":0}
